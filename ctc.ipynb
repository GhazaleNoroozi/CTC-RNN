{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install mido\n",
    "# !pip install git+https://github.com/KinWaiCheuk/AudioLoader.git\n",
    "# !pip install torchaudio\n",
    "# !pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all data at ./YourFolder\\TIMIT\\data\\TRAIN\n"
     ]
    }
   ],
   "source": [
    "from AudioLoader.speech import TIMIT\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio.transforms as T\n",
    "import torch\n",
    "\n",
    "# AudioLoader helps you to set up supported datasets\n",
    "dataset = TIMIT('./YourFolder',\n",
    "                split='train',\n",
    "                groups=[1,1],\n",
    "                download=False)\n",
    "train_loader = DataLoader(dataset,\n",
    "                          batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(waveform, sample_rate):\n",
    "    frame_length_ms = 25\n",
    "    hop_length_ms = 10\n",
    "    num_filterbank = 26\n",
    "    num_mfcc = 12\n",
    "    n_fft = 256\n",
    "\n",
    "    frame_length_samples = int(sample_rate * (frame_length_ms / 1000))\n",
    "    hop_length_samples = int(sample_rate * (hop_length_ms / 1000))\n",
    "\n",
    "    # Create Mel Filterbank transform\n",
    "    mel_filterbank = T.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length_samples,\n",
    "        n_mels=num_filterbank\n",
    "    )\n",
    "\n",
    "    # Create MFCC transform\n",
    "    mfcc_transform = T.MFCC(\n",
    "        sample_rate=sample_rate,\n",
    "        n_mfcc=num_mfcc,\n",
    "        melkwargs={\"n_fft\": n_fft, \"hop_length\": hop_length_samples, \"n_mels\": num_filterbank}\n",
    "    )\n",
    "\n",
    "    # Process the waveform in frames\n",
    "    filterbank_features = []\n",
    "    mfccs = []\n",
    "\n",
    "    # Iterate through the waveform\n",
    "    for start in range(0, waveform.size(1) - frame_length_samples + 1, hop_length_samples):\n",
    "        frame = waveform[:, start:start + frame_length_samples]\n",
    "\n",
    "        # Extract filterbank features\n",
    "        mel_spectrogram = mel_filterbank(frame)\n",
    "        filterbank_features.append(mel_spectrogram)\n",
    "        \n",
    "        # Extract MFCCs\n",
    "        mfcc = mfcc_transform(frame)\n",
    "        mfccs.append(mfcc)\n",
    "\n",
    "    # Stack all features\n",
    "    filterbank_features = torch.stack(filterbank_features, dim=0)\n",
    "    mfccs = torch.stack(mfccs, dim=0)\n",
    "    mfccs_reshaped = mfccs.squeeze(1)  # Remove batch dimension\n",
    "    # Select only MFCCs and delta (first derivative)\n",
    "    mfccs_final = torch.cat((mfccs_reshaped[:, :, 0], mfccs_reshaped[:, :, 1]), dim=1)\n",
    "    return mfccs_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([290, 24])\n"
     ]
    }
   ],
   "source": [
    "# Check the features of the dataset\n",
    "features = preprocess_audio(dataset[1]['waveform'], dataset[1]['sample_rate'])\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_phonemes = 'b d g p t k dx q jh ch s sh z zh f th v dh m n ng em en eng nx l r w y hh hv el iy ih eh ey ae aa aw ay ah ao oy ow uh uw ux er ax ix axr ax-h pau epi h# 1 2'.split(' ')\n",
    "alphabet = dict()\n",
    "for idx in range(len(all_phonemes)):\n",
    "  ph = all_phonemes[idx]\n",
    "  alphabet[ph] = idx + 1\n",
    "alphabet[' '] = 0\n",
    "closure_intervals = ['bcl', 'dcl', 'gcl', 'pcl', 'tcl', 'kcl', '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input shape:  torch.Size([4, 307, 24]) output shape:  torch.Size([4, 35])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def make_dataset(size):\n",
    "  input_tensor_list = []\n",
    "  output_tensor_list = []\n",
    "  for i in range(size):\n",
    "    input_tensor = preprocess_audio(dataset[2*i+1]['waveform'], dataset[2*i+1]['sample_rate'])\n",
    "    input_tensor_list.append(input_tensor)\n",
    "\n",
    "    phonemes = dataset[2*i-1]['phonemics']\n",
    "    phonemes_list = phonemes.split(' ')\n",
    "    output = [alphabet[ph] for ph in phonemes_list if ph not in closure_intervals]\n",
    "    output_tensor = torch.tensor(output)\n",
    "    output_tensor_list.append(output_tensor)\n",
    "\n",
    "  train_size = int(size*0.8)\n",
    "  I_train = pad_sequence(input_tensor_list[:train_size], batch_first=True)\n",
    "  O_train = pad_sequence(output_tensor_list[:train_size], batch_first=True)\n",
    "  I_test = pad_sequence(input_tensor_list[train_size:], batch_first=True)\n",
    "  O_test = pad_sequence(output_tensor_list[train_size:], batch_first=True)\n",
    "  return I_train, O_train, I_test, O_test\n",
    "\n",
    "I_train, O_train, I_test, O_test = make_dataset(5)\n",
    "print(\"Train input shape: \", I_train.shape, \"output shape: \", O_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # wights of hidden to output layer, output of this layer is logits\n",
    "        self.Why = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device) \n",
    "        \n",
    "        # Forward propagate the RNN\n",
    "        out, _ = self.rnn(x, h0)  # out: (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        # Pass the RNN output through the last layer \n",
    "        logits = self.Why(out)  # logits: (batch_size, seq_length, output_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class CTCLoss(nn.Module):\n",
    "  def __init__(self, logit_shape):\n",
    "    super(CTCLoss, self).__init__()\n",
    "    self.grads = torch.zeros(logit_shape)\n",
    "\n",
    "  def l_to_prime(self, l_tensor):\n",
    "    l_prime = l_tensor.repeat_interleave(2)\n",
    "    l_prime = torch.cat((torch.tensor([0]), l_prime), dim=0)\n",
    "    return l_prime\n",
    "  \n",
    "  def calc_ALPHA_BETA_Q(self, l, probs, b_idx = 0):\n",
    "    l_prime = self.l_to_prime(l)\n",
    "    len_l_prime = l_prime.shape[0] # 2 * |l| + 1\n",
    "    len_T = probs.shape[0] # T\n",
    "  \n",
    "    ALPHA = torch.zeros(len_l_prime, len_T)\n",
    "    BETA = torch.zeros(len_l_prime, len_T)\n",
    "    Q = torch.ones(len_T)\n",
    "\n",
    "    ALPHA[0, 0] = probs[0, b_idx]\n",
    "    ALPHA[1, 0] = probs[0, l[0].item()]\n",
    "    C_alpha_col = ALPHA[0, 0] + ALPHA[1, 0]\n",
    "    \n",
    "\n",
    "    BETA[-1, -1] = probs[len_T - 1, b_idx]\n",
    "    BETA[-2, -1] = probs[len_T - 1, l[-1].item()]\n",
    "    D_beta_col = BETA[-1, -1] + BETA[-2, -1]\n",
    "\n",
    "    ALPHA[0, 0] = ALPHA[0, 0]/C_alpha_col\n",
    "    ALPHA[1, 0] = ALPHA[1, 0]/C_alpha_col\n",
    "    BETA[-1, -1] = BETA[-1, -1] / D_beta_col\n",
    "    BETA[-2, -1] = BETA[-2, -1] / D_beta_col\n",
    "    Q[-1] = D_beta_col\n",
    "    for t in range(1, len_T):\n",
    "      alpha_col = t\n",
    "      beta_col = len_T - 1 - t\n",
    "      D_beta_col, C_alpha_col = 0, 0\n",
    "      start, end = max(0, len_l_prime - 2 * (len_T - t)), min(len_l_prime, 2 * (t + 1))\n",
    "\n",
    "      for s in range(start, end):\n",
    "        # Calculate ALPHA\n",
    "        max_idx = 0\n",
    "        if s - 1 >= 0 and l_prime[s] == b_idx:\n",
    "          max_idx = 1\n",
    "        elif (s - 2) >= 0 and l_prime[s] == l_prime[s - 2]:\n",
    "          max_idx = 1\n",
    "        elif (s - 2) >= 0:\n",
    "          max_idx = 2\n",
    "\n",
    "        alpha_y_val = probs[alpha_col, l_prime[s]]\n",
    "        alpha_bar = 0\n",
    "        for idx in range(max_idx + 1):\n",
    "          alpha_bar += ALPHA[s - idx, alpha_col - 1]\n",
    "        val = alpha_bar * alpha_y_val\n",
    "        ALPHA[s, alpha_col] = val\n",
    "        C_alpha_col += val\n",
    "\n",
    "        # Calculate BETA\n",
    "        max_idx = 0\n",
    "        if (s + 1) < len_l_prime and l_prime[s] == b_idx:\n",
    "          max_idx = 1\n",
    "        elif (s + 2) < len_l_prime and l_prime[s] == l_prime[s + 2]:\n",
    "          max_idx = 1\n",
    "        elif (s + 2) < len_l_prime:\n",
    "          max_idx = 2\n",
    "\n",
    "        beta_y_val = probs[beta_col, l_prime[s]]\n",
    "        beta_bar = 0\n",
    "        for idx in range(0, max_idx + 1):\n",
    "          beta_bar += BETA[s + idx, beta_col + 1]\n",
    "        val = beta_bar * beta_y_val\n",
    "        BETA[s, beta_col] = val\n",
    "        D_beta_col += val\n",
    "\n",
    "      Q[beta_col] *= D_beta_col\n",
    "      Q[alpha_col - 1] /= C_alpha_col\n",
    "\n",
    "      if C_alpha_col != 0:\n",
    "        ALPHA[:, alpha_col] = ALPHA[:, alpha_col] / C_alpha_col\n",
    "      if D_beta_col != 0:\n",
    "        BETA[:, beta_col] = BETA[:, beta_col] / D_beta_col\n",
    "    \n",
    "\n",
    "    for t in range(len_T - 2, -1, -1):\n",
    "      Q[t] *= Q[t + 1]\n",
    "    \n",
    "    return ALPHA, BETA, Q\n",
    "\n",
    "\n",
    "  def ctc_loss_gradient(self, probs, ALPHA, BETA, Q, z):\n",
    "    # for one target!\n",
    "    len_T = probs.shape[0]\n",
    "    len_alphabet = probs.shape[1]\n",
    "    grads = torch.zeros((len_T, len_alphabet))\n",
    "    for t in range(0, len_T):\n",
    "      for k in range(0, len(alphabet)):\n",
    "\n",
    "        k_in_z_indices = [i for i, c in enumerate(z) if c == k] # lab(z, k)\n",
    "        mult = 1\n",
    "        for s in k_in_z_indices:\n",
    "          mult *= ALPHA[s, t] * BETA[s, t]\n",
    "\n",
    "        y_t_k = probs[t, k]\n",
    "        grads[t, k] = y_t_k - Q[t]/y_t_k * mult\n",
    "    return grads\n",
    "  \n",
    "    \n",
    "  def loss(self, targets, probs, b_idx = 0):\n",
    "    grad_matrices = []\n",
    "    len_target = len(targets)\n",
    "    total_loss = 0\n",
    "    for i in range(len_target):\n",
    "      print(\"sequence number \", i)\n",
    "      z = targets[i]\n",
    "      ALPHA, BETA, Q = self.calc_ALPHA_BETA_Q(z, probs[i], b_idx)\n",
    "      total_loss += ALPHA[-1, -1].item() + ALPHA[-2, -1].item()\n",
    "\n",
    "      gradient = self.ctc_loss_gradient(probs[i], ALPHA, BETA, Q, z)\n",
    "      grad_matrices.append(gradient)\n",
    "    \n",
    "    self.grads = torch.stack(grad_matrices, dim=0) #(len_targets, T, L) the same as logits\n",
    "    return total_loss\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "sequence number  0\n",
      "sequence number  1\n",
      "sequence number  2\n",
      "sequence number  3\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "input_size = I_train.shape[2]     # Number of input features per time step\n",
    "batch_size = I_train.shape[1]     # Number of sequences in each batch\n",
    "train_size = I_train.shape[0]\n",
    "hidden_size = 256                 # Number of units in RNN's hidden layer\n",
    "output_size = len(alphabet)       # Number of output classes (including blank token)\n",
    "num_layers = 1                    # Number of RNN layers (can be increased)\n",
    "\n",
    "model = RNN(input_size, hidden_size, output_size, num_layers)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Stochastic Gradient Descent\n",
    "logits = model(I_train)           # logits: (batch_size, seq_length, output_size) = (|S|, T, len(alphabet) + 1)\n",
    "probs = F.softmax(logits, dim=2)  # same size (batch_size, seq_length, num_classes) =  = (|S|, T, len(alphabet) + 1)\n",
    "ctc = CTCLoss(logits.shape)\n",
    "loss = ctc.loss(O_train, probs)\n",
    "logits.grad = ctc.grads\n",
    "logits.backward(gradient=ctc.grads)\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
